{"cells":[{"cell_type":"code","execution_count":0,"outputs":[],"metadata":{"collapsed":false,"_kg_hide-input":false},"source":"import os \nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, roc_curve, confusion_matrix, classification_report, roc_auc_score, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, cross_validate, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport sys \nfrom sklearn.model_selection import learning_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, CategoricalNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_recall_curve, average_precision_score, make_scorer, precision_score, recall_score, f1_score\nfrom scipy import stats\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFECV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statistics import mean \n\n################ UTILITY FUNCTIONS ###################\n# display test scores\ndef t_test(results_array, model_names, score_name):\n    string=\"\"\n    for i in range(len(model_names) - 1):\n           for j in range(i, len(model_names)):\n               if i == j:\n                   continue\n               t, p = stats.ttest_ind(results_array[i], results_array[j], equal_var=False)\n               string += \"\\n\"+score_name\n               string += \"T_Test between {} & {}: T Value = {}, P Value = {}\".format(model_names[i], model_names[j], t, p)\n               if p>0.05:\n                   string += \"p-value>0.05 so there's NO significant difference between models.\"\n               else:\n                   string += \"p-value<=0.05 so there's A significant difference between models.\" \n    return string\n\n# plot ROC \ndef plot_roc(fpr,tpr,model_name,selector_name):\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(model_name + \"/\" + selector_name)\n    plt.legend(loc=\"lower right\")\n    \n################ NORMALIZATION ###################\ndf_original = pd.read_pickle('final_step2.pkl')\n\nX = df_original.loc[:, [\"n_EAR\", \n                    \"n_MAR\", \"n_MOE\", \"n_EC\",\n                    \"n_LEB\", \"n_SOP\", \"PERCLOS\", \"CLOSENESS\"]]\n\ny = df_original.loc[:, \"DROWSINESS\"]\n\n# normalize each columns\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled=scaler.transform(X)\n\n################ MODEL and SCORE DEFINITIONS ###################\nmodels = []\nmodels.append(('KNN-5', KNeighborsClassifier(n_neighbors=5, n_jobs=-1)))\nmodels.append(('CART-gini', DecisionTreeClassifier(criterion=\"gini\"))) \nmodels.append(('NB', GaussianNB()))\nmodels.append(('KNN-25', KNeighborsClassifier(n_neighbors=25, n_jobs=-1)))\nmodels.append(('CART-entropy', DecisionTreeClassifier(criterion=\"entropy\"))) \n\nscoring = []\nscoring.append(('accuracy', accuracy_score))\nscoring.append(('prec', precision_score))\nscoring.append(('recall', recall_score))\nscoring.append(('f1', f1_score))\nscoring.append(('auc', roc_auc_score))\n\n\n################ VARIABLES ###################\nfold_info_list = []\ntable1_output = \"\"\nfold_no = 0\n\n################ OUTER CV FOR T-TEST (5 FOLD) ###################\nouter_cv = StratifiedKFold(n_splits=5,random_state=42, shuffle=True)\nfor train_index, test_index in outer_cv.split(X_scaled, y): \n    X_train, X_test, y_train, y_test = X_scaled[train_index], X_scaled[test_index], y[train_index], y[test_index]\n    \n    ############# FEATURE SELECTION (4 METHODS: FULL, ANOVA, MI, RFE-RF) #############\n    table1_output += \"OUTER CV FOLD NO: {}\\n\".format(fold_no)\n    X_train_FULL = X_train\n    \n    ANOVA_selector = SelectKBest(f_classif, k=5)\n    X_train_ANOVA = ANOVA_selector.fit_transform(X_train, y_train)\n    print(\"ANOVA scores: {}, ANOVA p-values: {}\".format(ANOVA_selector.scores_, ANOVA_selector.pvalues_))\n    table1_output += \"ANOVA scores: {}, ANOVA p-values: {}\".format(ANOVA_selector.scores_, ANOVA_selector.pvalues_)\n    \n    MI_selector = SelectKBest(mutual_info_classif, k=5)\n    X_train_MI = MI_selector.fit_transform(X_train, y_train)\n    print(\"MI scores: {}, MI p-values: {}\".format(MI_selector.scores_, MI_selector.pvalues_))\n    table1_output += \"MI scores: {}, MI p-values: {}\".format(MI_selector.scores_, MI_selector.pvalues_)\n    \n    #estimator for recursive feature elimination\n    # estimator = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', class_weight= None, max_features = None, random_state = 42,n_jobs=-1)\n    estimator = DecisionTreeClassifier(random_state = 42)\n    #inner cv for recursive feature elimination\n    inner_cv = StratifiedShuffleSplit(n_splits=2,test_size=0.2, random_state = 42)\n    RFE_selector = RFECV(estimator, step=1, cv=inner_cv, n_jobs=-1)\n    X_train_RFE = RFE_selector.fit_transform(X_train, y_train)\n    print(\"RFE rankings: {}, RFE grid-scores: {}\".format(RFE_selector.ranking_, RFE_selector.grid_scores_))\n    table1_output += \"RFE rankings: {}, RFE grid-scores: {}\".format(RFE_selector.ranking_, RFE_selector.grid_scores_)\n    \n    ############# TEST RESULTS FOR FEATURE SELECTION METHODS #############\n    fold_info = {\n        model_name:{\n            'FULL': {\n                score_name:0 for score_name,score in scoring\n                },\n            'ANOVA': {\n                score_name:0 for score_name,score in scoring\n                },\n            'MI': {\n                score_name:0 for score_name,score in scoring\n                },\n            'RFE': {\n                score_name:0 for score_name,score in scoring\n                }\n            } for model_name, model in models\n        }\n    for model_name, model in models:\n        # transform X_test according to feature selection methods\n        X_test_FULL = X_test\n        X_test_ANOVA = ANOVA_selector.transform(X_test)\n        X_test_MI = MI_selector.transform(X_test)\n        X_test_RFE = RFE_selector.transform(X_test)\n        # make predictions\n        model.fit(X_train_FULL,y_train)\n        y_pred_FULL = model.predict(X_test_FULL)\n        model.fit(X_train_ANOVA,y_train)\n        y_pred_ANOVA = model.predict(X_test_ANOVA)\n        model.fit(X_train_MI,y_train)\n        y_pred_MI = model.predict(X_test_MI)\n        model.fit(X_train_RFE,y_train)\n        y_pred_RFE = model.predict(X_test_RFE)\n        # save evaluation metrics\n        for score_name, score in scoring:\n            score_FULL = score(y_test,y_pred_FULL)\n            score_ANOVA = score(y_test,y_pred_ANOVA)\n            score_MI = score(y_test,y_pred_MI)\n            score_RFE = score(y_test,y_pred_RFE)\n            fold_info[model_name]['FULL'][score_name] = score_FULL,\n            fold_info[model_name]['ANOVA'][score_name] = score_ANOVA,\n            fold_info[model_name]['MI'][score_name] = score_MI,\n            fold_info[model_name]['RFE'][score_name] = score_RFE,\n\n        # save ROC metrics\n        fpr_FULL, tpr_FULL, _ = roc_curve(y_test,y_pred_FULL)\n        fold_info[model_name]['FULL']['ROC_fpr'] = fpr_FULL\n        fold_info[model_name]['FULL']['ROC_tpr'] = tpr_FULL\n        fpr_ANOVA, tpr_ANOVA, _ = roc_curve(y_test,y_pred_ANOVA)\n        fold_info[model_name]['ANOVA']['ROC_fpr'] = fpr_ANOVA\n        fold_info[model_name]['ANOVA']['ROC_tpr'] = tpr_ANOVA\n        fpr_MI, tpr_MI, _ = roc_curve(y_test,y_pred_MI)\n        fold_info[model_name]['MI']['ROC_fpr'] = fpr_MI\n        fold_info[model_name]['MI']['ROC_tpr'] = tpr_MI\n        fpr_RFE, tpr_RFE, _ = roc_curve(y_test,y_pred_RFE)\n        fold_info[model_name]['RFE']['ROC_fpr'] = fpr_RFE\n        fold_info[model_name]['RFE']['ROC_tpr'] = tpr_RFE\n        \n        # save confusion matrix\n        conf_FULL = confusion_matrix(y_test,y_pred_FULL)\n        fold_info[model_name]['FULL']['confusion_matrix'] = conf_FULL\n        conf_ANOVA = confusion_matrix(y_test,y_pred_ANOVA)\n        fold_info[model_name]['ANOVA']['confusion_matrix'] = conf_ANOVA\n        conf_MI = confusion_matrix(y_test,y_pred_MI)\n        fold_info[model_name]['MI']['confusion_matrix'] = conf_MI\n        conf_RFE = confusion_matrix(y_test,y_pred_RFE)\n        fold_info[model_name]['RFE']['confusion_matrix'] = conf_RFE\n            \n            \n    fold_info_list.append(fold_info)\n    fold_no += 1 \n\n############# T-TEST #############\n\nresults = {\n    \"FULL\": {score_name:[] for score_name,score in scoring},\n    \"ANOVA\": {score_name:[] for score_name,score in scoring},\n    \"MI\": {score_name:[] for score_name,score in scoring},\n    \"RFE\": {score_name:[] for score_name,score in scoring},\n    }\n\n# build result arrays for corresponding metrics\nfor score_name, score in scoring:\n    for model_name, model in models:\n        model_results_FULL = []\n        model_results_ANOVA = []\n        model_results_MI = []\n        model_results_RFE = []\n        for fold_info in fold_info_list:\n             model_results_FULL.append(fold_info[model_name]['FULL'][score_name])\n             model_results_ANOVA.append(fold_info[model_name]['ANOVA'][score_name])\n             model_results_MI.append(fold_info[model_name]['MI'][score_name])\n             model_results_RFE.append(fold_info[model_name]['RFE'][score_name])\n        results['FULL'][score_name].append(model_results_FULL)\n        results['ANOVA'][score_name].append(model_results_ANOVA)\n        results['MI'][score_name].append(model_results_MI)\n        results['RFE'][score_name].append(model_results_RFE)\n\n# calculate t-test on 5 folds for corresponding model and metric\nt_test_output=\"\"\nmodel_names = [model_name for model_name, model in models]\nfor score_name, score in scoring:\n        t_test_output += '\\nFULL'\n        results_array = results['FULL'][score_name]\n        t_test_output += t_test(results_array, model_names, score_name)\n        t_test_output += '\\nANOVA'\n        results_array = results['ANOVA'][score_name]\n        t_test_output += t_test(results_array, model_names, score_name)\n        t_test_output += '\\nMI'\n        results_array = results['MI'][score_name]\n        t_test_output += t_test(results_array, model_names, score_name)\n        t_test_output += '\\nRFE'\n        results_array = results['RFE'][score_name]\n        t_test_output += t_test(results_array, model_names, score_name)\n\ntextfile = open('t-test.txt', 'w')\ntextfile.write(t_test_output)\ntextfile.close()   \n\n############ EVALUATION METRICS #####################\n\nmetrics_output = \"\"\nfor model_name, model in models:\n    metrics_output += \"\\n{} with full features\\n\".format(model_name)\n    for score_name, score in scoring:\n        metrics_output += \"{}: {},{},{},{},{}, average: {}\".format(score_name, *[fold_info[model_name][\"FULL\"][score_name][0] for fold_info in fold_info_list], mean([fold_info[model_name][\"FULL\"][score_name][0] for fold_info in fold_info_list]))\n    metrics_output += \"\\n{} with ANOVA\\n\".format(model_name)\n    for score_name, score in scoring:\n        metrics_output += \"{}: {},{},{},{},{}, average: {}\".format(score_name, *[fold_info[model_name][\"ANOVA\"][score_name][0] for fold_info in fold_info_list], mean([fold_info[model_name][\"ANOVA\"][score_name][0] for fold_info in fold_info_list]))\n    metrics_output += \"\\n{} with MI\\n\".format(model_name)\n    for score_name, score in scoring:\n        metrics_output += \"{}: {},{},{},{},{}, average: {}\".format(score_name, *[fold_info[model_name][\"MI\"][score_name][0] for fold_info in fold_info_list], mean([fold_info[model_name][\"MI\"][score_name][0] for fold_info in fold_info_list]))\n    metrics_output += \"\\n{} with RFE\\n\".format(model_name)\n    for score_name, score in scoring:\n        metrics_output += \"{}: {},{},{},{},{}, average: {}\".format(score_name, *[fold_info[model_name][\"RFE\"][score_name][0] for fold_info in fold_info_list], mean([fold_info[model_name][\"RFE\"][score_name][0] for fold_info in fold_info_list]))\n\ntextfile2 = open('metrics.txt', 'w')\ntextfile2.write(metrics_output)\ntextfile2.close()   \n\n\n\n\nmetrics_output2 = \"\"\nfor model_name, model in models:\n    metrics_output += \"\\n{} with full features\\n\".format(model_name)\n    for score_name, score in scoring:\n        metrics_output += \"{}: {}\".format(score_name, mean([fold_info[model_name][\"FULL\"][score_name] for fold_info in fold_info_list]))\n    metrics_output += \"\\n{} with ANOVA\\n\".format(model_name)\n    for score_name, score in scoring:\n        metrics_output += \"{}: {}\".format(score_name, mean([fold_info[model_name][\"ANOVA\"][score_name][0] for fold_info in fold_info_list]))\n    metrics_output += \"\\n{} with MI\\n\".format(model_name)\n    for score_name, score in scoring:\n        metrics_output += \"{}: {}\".format(score_name, mean([fold_info[model_name][\"MI\"][score_name][0] for fold_info in fold_info_list]))\n    metrics_output += \"\\n{} with RFE\\n\".format(model_name)\n    for score_name, score in scoring:\n        metrics_output += \"{}: {}\".format(score_name, mean([fold_info[model_name][\"RFE\"][score_name][0] for fold_info in fold_info_list]))\n\ntextfile3 = open('metrics2.txt', 'w')\ntextfile3.write(metrics_output2)\ntextfile3.close()   \n        \n        \n# ############ ROC CURVES #########################\n# # plot curves in grid\n# plt.figure(figsize=(20,20))\n# for i in range(0, 5):\n#     plt.subplot(5, 4, 4*i+1)\n#     fpr = np.mean([fold_info[model_names[i]]['FULL']['ROC_fpr'] for fold_info in fold_info_list], axis=0)\n#     tpr = np.mean([fold_info[model_names[i]]['FULL']['ROC_tpr'] for fold_info in fold_info_list], axis=0)\n#     plot_roc(fpr,tpr,model_names[i],\"FULL features\")\n    \n#     plt.subplot(5, 4, 4*i+2)\n#     fpr = np.mean([fold_info[model_names[i]]['ANOVA']['ROC_fpr'] for fold_info in fold_info_list], axis=0)\n#     tpr = np.mean([fold_info[model_names[i]]['ANOVA']['ROC_tpr'] for fold_info in fold_info_list], axis=0)\n#     plot_roc(fpr,tpr,model_names[i],\"ANOVA selector\")\n    \n#     plt.subplot(5, 4, 4*i+3)\n#     fpr = np.mean([fold_info[model_names[i]]['MI']['ROC_fpr'] for fold_info in fold_info_list], axis=0)\n#     tpr = np.mean([fold_info[model_names[i]]['MI']['ROC_tpr'] for fold_info in fold_info_list], axis=0)\n#     plot_roc(fpr,tpr,model_names[i],\"Mutual Info selector\")\n    \n#     plt.subplot(5, 4, 4*i+4)\n#     fpr = np.mean([fold_info[model_names[i]]['RFE']['ROC_fpr'] for fold_info in fold_info_list], axis=0)\n#     tpr = np.mean([fold_info[model_names[i]]['RFE']['ROC_tpr'] for fold_info in fold_info_list], axis=0)\n#     plot_roc(fpr,tpr,model_names[i],\"RFE selector with RF\")\n    \n    \n        \n# ############ CONFUSION MATRICES #########################    \n# sns.set(font_scale=2.0) # for label size\n\n\n# for no, fold_info in enumerate(fold_info_list,1):\n#     plt.figure(figsize=(20,20))\n#     plt.subplots_adjust(wspace = 0.5, hspace = 0.5)\n\n#     # plt.title(\"FOLD NO: {}\".format(no))\n#     for i in range(0, 5):\n#         plt.subplot(5, 4, 4*i+1)\n#         plt.title(model_names[i] + \"/\" + \"FULL\")\n#         df_cm = pd.DataFrame(fold_info[model_names[i]]['FULL']['confusion_matrix'])\n#         sns.heatmap(df_cm, annot=True, fmt=\"d\",cbar=False, annot_kws={\"size\": 18}) # font size\n        \n#         plt.subplot(5, 4, 4*i+2)\n#         plt.title(model_names[i] + \"/\" + \"ANOVA\")\n#         df_cm = pd.DataFrame(fold_info[model_names[i]]['ANOVA']['confusion_matrix'])\n#         sns.heatmap(df_cm, annot=True, fmt=\"d\",cbar=False, annot_kws={\"size\": 18}) # font size\n        \n#         plt.subplot(5, 4, 4*i+3)\n#         plt.title(model_names[i] + \"/\" + \"MI\")\n#         df_cm = pd.DataFrame(fold_info[model_names[i]]['MI']['confusion_matrix'])\n#         sns.heatmap(df_cm, annot=True, fmt=\"d\",cbar=False, annot_kws={\"size\": 18}) # font size\n        \n#         plt.subplot(5, 4, 4*i+4)\n#         plt.title(model_names[i] + \"/\" + \"RFE\")\n#         df_cm = pd.DataFrame(fold_info[model_names[i]]['RFE']['confusion_matrix'])\n#         sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, annot_kws={\"size\": 18}) # font size\n\n\n\n\n\n\n"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}