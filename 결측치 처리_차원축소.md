![image-20210610144107089](C:\Users\j\AppData\Roaming\Typora\typora-user-images\image-20210610144107089.png)

## 결측치 처리 방법



```python
from sklearn.impute import SimpleImputer
# 결측치 처리 (평균값 = 기본값, 최빈값 = most frequent, 중앙값 = median)
imputer = SimpleImputer(strategy='most_frequent')
```

SimpleImputer() 를 통해서 결측치를 replace한다.



## 특징 추출

### RFE(Recursive Feature Elimination)

```python
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
```

sklearn의 RFE 패키지를 사용하여 데이터의 차원을 축소한다

RFE 패키지는 데이터의 차원축소 랭킹을 매겨서 리턴한다.

```python
rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) # 5개로 차원을 축소

# fit RFE
rfe.fit(X, y)

# 랭킹 출력
for i in range(X.shape[1]):
  print('Column: %d, Selected=%s, Rank: %d' % (i, rfe.support_[i], rfe.ranking_[i]))
```

```
<출력결과>

Column: 0, Selected=False, Rank: 4
Column: 1, Selected=False, Rank: 5
Column: 2, Selected=True, Rank: 1
Column: 3, Selected=True, Rank: 1
Column: 4, Selected=True, Rank: 1
Column: 5, Selected=False, Rank: 6
Column: 6, Selected=True, Rank: 1
Column: 7, Selected=False, Rank: 2
Column: 8, Selected=True, Rank: 1
Column: 9, Selected=False, Rank: 3
```



### PCA

```python
from sklearn.decomposition import PCA
```

sklearn의 pca 함수를 사용하여 데이터의 차원을 축소한다.

rfe와 다르게 pca함수는 데이터의 차원을 축소하여 출력해준다.

```python
# 차원(특징)을 5개로 축소
trans = PCA(n_components=5)
X_dim = trans.fit_transform(X)

print(X_dim[:3, :])
```

```
<출력결과>

[[-1.64710578e+00 -2.11683302e+00  1.98256096e+00  1.96848970e-16
   5.62167863e-16]
 [ 9.28402085e-01  4.82949970e+00  2.27270432e-01  2.18601301e-16
   8.07228007e-16]
 [-3.83677757e+00  3.23007138e-01  1.15128013e-01  4.31431934e-16
   6.31900462e-16]]
```



### Regression Feature Selection

Numerical Input, Numerical Output 일 때 사용가능하다

```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
```

f_regression은 anova의 f-value값을 이용한다

```python
# 차원을 10개로 축소
fs = SelectKBest(score_func=f_regression, k=10)

X_selected = fs.fit_transform(X, y)
```

